{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4454e486",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Analysis of the instances of \"yo\" in NLTK's Twitter dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e12875",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "For my final project, I have developed a program that uses the Twitter sampleset from NLTK to identify instances of the word \"yo\" and how it is used in social media interactions. \n",
    "\n",
    "The objective of the project is to examine the sentiment associated with \"yo\", identify the most commonly used parts of speech of \"yo\", and determine the typical enviornments in which it is used.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28f1ad4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "My project contains three main parts:\n",
    "\n",
    "First, the program calculates the sentiment score of each instance of \"yo\" using the SentimentIntensityAnalyzer from NLTK and stores the scores in a list. The code then calculates the average sentiment score of all \"yo\" instances.\n",
    "\n",
    "The program then uses the POS tagger from NLTK to tag the words in each tweet and determine the POS of each instance of \"yo\". It then creates a frequency distribution of the POS tags for all instances of \"yo\".\n",
    "\n",
    "Finally, the program creates a list of bigrams of the words that appear before and after each instance of \"yo\" in the dataset paired with their frequency distributions. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "78b21f97",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total instances of 'yo': 13\n"
     ]
    }
   ],
   "source": [
    "#I imported NLTK and created a variable containing the Twitter dataset:\n",
    "import nltk\n",
    "tweets = nltk.corpus.twitter_samples.strings()\n",
    "# List to store all instances of \"yo\"\n",
    "yo_instances = []\n",
    "\n",
    "#For loop tokenizing the tweets into individual words\n",
    "for tweet in tweets:\n",
    "    words = nltk.word_tokenize(tweet)\n",
    "    \n",
    "#Another loop for each word in the tweets to store instances of \"yo\" in a list\n",
    "    for i in range(len(words)):\n",
    "        if words[i].lower() == \"yo\":\n",
    "            yo_instances.append(tweet)\n",
    "\n",
    "# Print number of instances of \"yo\"\n",
    "print(\"Total instances of 'yo':\", len(yo_instances))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e41aab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "I was very disappointed in the number of instances of \"yo\" found in the corpus. \n",
    "\n",
    "I was expecting to find more and considered extracting data from the Twitter API, or even Reddit API, but decided to stick with my orignal plan to avoid more confusion and overcomplicating my project to a level higher than what I felt I was capable of carrying out succesfully in the time allotted.\n",
    "\n",
    "For future exploration, I would like to incorporate a larger dataset to get a more accurate conclusion of the ways in which \"yo\" is used in social media interaction.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be9bab0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "However, I was still able to implement all parts of my project with the data available to me through the NLTK library. Also, because the dataset was so small, I was able to check for accuracy in the program's outputs. I can efficiently sort through the smaller set of data myself and see how my analysis compares to the way my program analyzes the corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d07788",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Part 1:\n",
    "\n",
    "Part one consists of calculating the sentiment analysis score of the instances in which \"yo\" appears in the provided tweets from the corpus. This was the most difficult process. \n",
    "\n",
    "I initially planned to use NaÃ¯ve Bayes, but ultimately decided to explore more options for sentiment analysis in NLTK. I used NLTK's Sentiment Intensity Analyzer to determine the sentiment score of the instances of \"yo\" for my project. The SIA is a tool that uses a lexicon based approach to determine the sentiment (positive, negative, neutral) of a text. The lexicon is used to calculate a sentiment score that ranges from -1 (the most negative) to +1 (the most positive) and 0 to indicate a neutral sentiment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "845b5ddb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentiment score of 'yo' instances: 0.11000769230769232\n"
     ]
    }
   ],
   "source": [
    "#I downloaded the resources necessary for the Sentiment Intensity Analyzer for calculating the sentiment score.\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "# Function to calculate the sentiment score\n",
    "def get_sentiment_score(sentence):\n",
    "    analyzer = nltk.sentiment.SentimentIntensityAnalyzer()\n",
    "    # Calculate sentiment score of sentence in tweet\n",
    "    score = analyzer.polarity_scores(sentence)\n",
    "    # Return the compound score of value between -1 and 1\n",
    "    return score[\"compound\"]\n",
    "\n",
    "# Calculate sentiment score of \"yo\" instances and store in list\n",
    "sentiment_scores = []\n",
    "for instance in yo_instances:\n",
    "    sentiment_scores.append(get_sentiment_score(instance))\n",
    "\n",
    "# Calculate average sentiment score of \"yo\" instances\n",
    "avg_sentiment_score = sum(sentiment_scores) / len(sentiment_scores)\n",
    "\n",
    "# Print average sentiment score of \"yo\" instances\n",
    "print(\"Average sentiment score of 'yo' instances:\", avg_sentiment_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c062788",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The sentiment score of 0.11000769230769232 indicates that \"yo\" is used in a *slightly* positive way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d952ac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "However, I did my own sentiment score after analyzing the tweets based on what I thought to be positive, negative, and neutral sentiment in the tweets.\n",
    "\n",
    "My sentiment analysis of \"yo\":\n",
    "Positive: 4 instances\n",
    "Negative: 3 instances\n",
    "Neutral: 6 instances\n",
    "\n",
    "sentiment score = (4(pos)*0.5 + 3(neg)*(-0.5) + 6(neut)*0) / 13(total) \n",
    "sentiment score = (2 - 1.5) / 13 \n",
    "sentiment score = 0.0385\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f86717",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Although from what I understand, the SentimentIntensityAnalyzer has a more sophisticated and complex method for assigning sentiment scores to text than my simplfied approach (I chose to weight all the instances of positive and negative at .5 for positive and -.5 for negative because -1 is *extremely* negative and 1 is *extremely* positive), I did find that both my own analysis and NLTK's analysis both produced results that indicated that \"yo\" is overall used in slightly more positive settings based on the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e09673",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sources used to implement NLTK's SIA:\n",
    "https://www.nltk.org/api/nltk.sentiment.html\n",
    "\n",
    "https://www.nltk.org/_modules/nltk/sentiment/sentiment_analyzer.html#SentimentAnalyzer\n",
    "\n",
    "https://www.nltk.org/howto/sentiment.html\n",
    "\n",
    "https://realpython.com/python-nltk-sentiment-analysis/#using-nltks-pre-trained-sentiment-analyzer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd033a5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Part 2:\n",
    "\n",
    "Part two consists of counting the POS frequency of each instance of \"yo\". The POS tagger was very inaccurate and had a lot of trouble distinguishing the parts of speech of the instances in which \"yo\" is used in the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "919cdb95",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'yo' = NOUN in the sentence: Sometimes it be's like that, yo. Follow someone and then a few days later realise they're problematic as fuck. Life :(\n",
      "'yo' = NOUN in the sentence: @KEEMSTARx yo dude. Fancy helping a fan out? I wanna grow a yt channel but can't purchase an Elgato :(\n",
      "'yo' = NOUN in the sentence: @jenniferseon yo stop bragging not everybody got the skrillah to travel :(\n",
      "'yo' = ADJ in the sentence: @aminadujean oh no :( is there anything yo ucan do?\n",
      "'yo' = NOUN in the sentence: @jiarpi20 minal aidzin yo pi :D\n",
      "'yo' = ADJ in the sentence: Woke up yo @trentowers fav' my tweet this made my day :))) now just @luketurner89 ðŸ˜‚ðŸ˜‚ http://t.co/oGw5sVij7G\n",
      "'yo' = NOUN in the sentence: Can't date someone white. sorry. Yo grandpa prolly wanna burn me. :)\n",
      "'yo' = NOUN in the sentence: @HollyyLive yo yall should invite me for ranked :))))\n",
      "'yo' = NOUN in the sentence: @wuthering_alice I like that it doesn't have to be about consumerism :) Treat Yo Soul\n",
      "'yo' = NOUN in the sentence: @TMobile Yo gimme this month free on my bill since service has been out pretty much every other day :))))))))))))))))))))))))))\n",
      "'yo' = NOUN in the sentence: Yo Southpaw was a GREAT movie someone better be getting an award for it :D\n",
      "'yo' = NOUN in the sentence: Yo @DUPleader you must have missed #bbcqt earlier, both Labour and the Tories ruled out a coalition\n",
      "\n",
      "Tell us more about this unique position\n",
      "'yo' = NOUN in the sentence: RT @shawnoftheshred: psst: @sledisland just announced De La Soul, Yo La Tengo, Mykki Blanco, Colleen Green, Tory Lanez + 150 more to alreadâ€¦\n"
     ]
    }
   ],
   "source": [
    "# POS tagging using the universal tagset on each instance of \"yo\"\n",
    "for instance in yo_instances:\n",
    "    tokens = nltk.word_tokenize(instance)\n",
    "    yo_tags = [tag for (word, tag) in nltk.pos_tag(tokens, tagset='universal') if word.lower() == 'yo']\n",
    "    if yo_tags:\n",
    "#Prints the tag and tweet that \"yo\" occurred in to analyze the accuracy of the POS tagger\n",
    "        print(f\"'yo' = {yo_tags[0]} in the sentence: {instance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8fa7bc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Due to the dataset being as small as it is, I am able to determine the POS myself and compare my accurate POS tag (an interjection, most often) to what NLTK determined the POS tag was for each instance (typically a noun). \n",
    "\n",
    "My analysis:\n",
    "Interjection: 7\n",
    "Typo: 2\n",
    "Pronoun: 2\n",
    "Proper noun: 1\n",
    "Different language: 1 \n",
    "\n",
    "NLTK POS Tagger: \n",
    "Noun: 11\n",
    "Adj: 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f466bd6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "Although the POS tagger does well with many words, I predict that words that are not used as frequently, words that could be considered \"slang\", and words that have several different parts of speech depending on their use will result in inaccurate tags.\n",
    "\n",
    "However, with more data, a more accurate conclusion can be made.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc39aff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Part 3:\n",
    "\n",
    "Part three lists the bigrams before and after ocurrences of \"yo\" to show the enviornments in which the word typically occurs. \n",
    "\n",
    "My purpose for doing this is to see if there are any commonalities across the enviornments. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "66a426ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams before 'yo':\n",
      "[(('keemstarx', 'yo'), 1), (('jenniferseon', 'yo'), 1), (('anything', 'yo'), 1), (('aidzin', 'yo'), 1), (('hollyylive', 'yo'), 1), (('treat', 'yo'), 1), (('tmobile', 'yo'), 1)]\n",
      "Bigrams after 'yo':\n",
      "[(('yo', 'dude'), 1), (('yo', 'stop'), 1), (('yo', 'ucan'), 1), (('yo', 'pi'), 1), (('yo', 'yall'), 1), (('yo', 'soul'), 1), (('yo', 'gim'), 1)]\n"
     ]
    }
   ],
   "source": [
    "yo = 'yo'\n",
    "\n",
    "# List of bigrams for words that occur before and after \"yo\"\n",
    "before_yo_bigrams = []\n",
    "after_yo_bigrams = []\n",
    "for tweet in tweets:\n",
    "    tweet_tokens = nltk.word_tokenize(tweet.lower())\n",
    "    for i in range(1, len(tweet_tokens)-1):\n",
    "        if tweet_tokens[i] == yo and tweet_tokens[i-1].isalnum() and tweet_tokens[i+1].isalnum():\n",
    "            before_yo_bigrams.append((tweet_tokens[i-1], yo))\n",
    "            after_yo_bigrams.append((yo, tweet_tokens[i+1]))\n",
    "\n",
    "# Print bigrams and frequency distribution\n",
    "before_yo_fd = nltk.FreqDist(before_yo_bigrams)\n",
    "after_yo_fd = nltk.FreqDist(after_yo_bigrams)\n",
    "print(f\"Bigrams before '{yo}':\")\n",
    "print(before_yo_fd.most_common())\n",
    "print(f\"Bigrams after '{yo}':\")\n",
    "print(after_yo_fd.most_common())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af10e04e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Based on the outputs, I found that \"yo\" often follows someone's username. On Twitter, when addressing someone, you use @theirusername, which was often the case with this dataset. Therefore,\"yo\", based on this dataset, is commonly being used as a greeting/interjection in social media interactions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6c8d11",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "Overall, this project has allowed me to see how valuable of a tool programming can be for analyzing datasets, but it can also generate inaccuracies that can result in incorrect conclusions. Moving forward, I believe that incorporating larger datasets, accounting for differences in dialects, and updating the lexicon are instrumental for improving the accuracy and efficacy of data analysis.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
